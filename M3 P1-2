# -*- coding: utf-8 -*-
"""M3_1-2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Zh2BqiuyfCPqqPlyltMdQpjqEvUyZo0
"""

!pip -q install pandas numpy scipy scikit-learn tabulate implicit==0.7.2

##importanting the needed libraries
import os, math, time, json, gzip, pickle
from collections import defaultdict
from typing import List, Dict, Set
import numpy as np
import pandas as pd
from scipy import sparse
from tabulate import tabulate
import matplotlib.pyplot as plt

import implicit

DATA_DIR = "/content"
RATINGS_FILE = os.path.join(DATA_DIR, "ratings.csv")
MOVIES_FILE  = os.path.join(DATA_DIR, "movies.csv")
LINKS_FILE   = os.path.join(DATA_DIR, "links.csv")

K = 20
MIN_INTERACTIONS = 5
EVAL_USERS_CAP = 5000
RATING_THRESHOLD = 3.5

ALS_FACTORS = 64
ALS_REG     = 0.01
ALS_ITERS   = 15

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

OUT_DIR = "/content/out_eval"
REGISTRY = "/content/model_registry/v0.1"
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(REGISTRY, exist_ok=True)

print("Looking for files:")
for p in [RATINGS_FILE, MOVIES_FILE, LINKS_FILE]:
    print( p, "exists?" , os.path.exists(p))

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import pandas as pd, numpy as np, time

np.random.seed(42)
users  = np.arange(1, 11)
movies = np.arange(1, 21)

rows = []
for u in users:
    rated = np.random.choice(movies, size=np.random.randint(5, 15), replace=False)
    for m in rated:
        rows.append({
            "userId": u,
            "movieId": m,
            "rating": np.random.randint(1, 6),
            "timestamp": int(time.time()) - np.random.randint(0, 100000)
        })

df = pd.DataFrame(rows)
df.to_csv("/content/ratings.csv", index=False)
print("The ratings.csv:", df.shape)
df.head()

def load_ratings(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError("Upload ratings.csv to /content.")
    df = pd.read_csv(path)
    need = {"userId","movieId","rating","timestamp"}
    if not need.issubset(df.columns):
        raise ValueError(f"ratings.csv must contain {need}, got {df.columns.tolist()}")
    df["timestamp"] = df["timestamp"].astype(int)
    return df

def load_movies(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        print("Warning: movies.csv not found â€” genre slices will be skipped.")
        return pd.DataFrame()
    return pd.read_csv(path)

ratings = load_ratings(RATINGS_FILE)
movies  = load_movies(MOVIES_FILE)

print("Ratings the shape:", ratings.shape)
ratings.head()

cutoff_ts = np.quantile(ratings["timestamp"].values, 0.8)
train_df  = ratings[ratings["timestamp"] <= cutoff_ts].copy()
test_df   = ratings[ratings["timestamp"] >  cutoff_ts].copy()

train_counts = train_df.groupby("userId")["movieId"].count()
eligible_users = set(train_counts[train_counts >= MIN_INTERACTIONS].index)
train_items = set(train_df["movieId"].unique())

test_df = test_df[test_df["userId"].isin(eligible_users)]
test_df = test_df[test_df["movieId"].isin(train_items)]

if len(eligible_users) > EVAL_USERS_CAP:
    keep = set(np.random.choice(list(eligible_users), size=EVAL_USERS_CAP, replace=False))
    train_df = train_df[train_df["userId"].isin(keep)]
    test_df  = test_df[test_df["userId"].isin(keep)]
    eligible_users = keep

print(f"The Cutoff ts: {cutoff_ts}  (UTC {pd.to_datetime(cutoff_ts, unit='s', utc=True)})")
print("The Train and Test sizes:", train_df.shape, test_df.shape)
print("The Number of Eligible users:", len(eligible_users))

train_pos = train_df[train_df["rating"] >= RATING_THRESHOLD].copy()
user_ids = np.sort(train_pos["userId"].unique())
item_ids = np.sort(train_pos["movieId"].unique())
uid2idx = {u:i for i,u in enumerate(user_ids)}
iid2idx = {m:i for i,m in enumerate(item_ids)}

rows = train_pos["userId"].map(uid2idx).values
cols = train_pos["movieId"].map(iid2idx).values
data = np.ones_like(rows, dtype=np.float32)

user_item = sparse.coo_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids))).tocsr()
print("The CSR shape:", user_item.shape, "nnz:", user_item.nnz)

als = implicit.als.AlternatingLeastSquares(
    factors=ALS_FACTORS, regularization=ALS_REG, iterations=ALS_ITERS, random_state=RANDOM_SEED
)
als.fit(user_item.T)

with open(os.path.join(REGISTRY, "uid2idx.pkl"), "wb") as f: pickle.dump(uid2idx, f)
with open(os.path.join(REGISTRY, "iid2idx.pkl"), "wb") as f: pickle.dump(iid2idx, f)
np.save(os.path.join(REGISTRY, "user_factors.npy"), als.user_factors)
np.save(os.path.join(REGISTRY, "item_factors.npy"), als.item_factors)
print("The Model to Saved to", REGISTRY)

def precision_at_k(recs: List[int], truth: Set[int], k: int) -> float:
    if k == 0: return 0.0
    return sum(1 for x in recs[:k] if x in truth) / k
def recall_at_k(recs: List[int], truth: Set[int], k: int) -> float:
    if not truth: return 0.0
    return sum(1 for x in recs[:k] if x in truth) / len(truth)
def dcg_at_k(recs: List[int], truth: Set[int], k: int) -> float:
    dcg = 0.0
    for i, item in enumerate(recs[:k], start=1):
        if item in truth:
            dcg += 1.0 / math.log2(i+1)
    return dcg
def ndcg_at_k(recs: List[int], truth: Set[int], k: int) -> float:
    ideal_hits = min(len(truth), k)
    if ideal_hits == 0: return 0.0
    ideal_dcg = sum(1.0 / math.log2(i+1) for i in range(1, ideal_hits+1))
    return dcg_at_k(recs, truth, k) / ideal_dcg
def apk(recs: List[int], truth: Set[int], k: int) -> float:
    hits, s = 0, 0.0
    for i, item in enumerate(recs[:k], start=1):
        if item in truth:
            hits += 1
            s += hits / i
    if not truth: return 0.0
    return s / min(len(truth), k)

def recommend_items_for_uid(u_external: int, k: int = 100) -> list:
    if u_external not in uid2idx:
        return list(item_ids[:k])
    u = uid2idx[u_external]
    user_vector = user_items_csr[u]
    if user_vector.shape[0] != 1:
        user_vector = user_vector.reshape(1, -1)
    recs = als.recommend(u, user_items_csr, N=k, filter_already_liked_items=True)
    return [int(item_ids[iid]) for iid, _ in recs]

test_pos = test_df[(test_df["rating"] >= RATING_THRESHOLD) & (test_df["movieId"].isin(item_ids))].copy()
from collections import defaultdict
truth_by_user: Dict[int, Set[int]] = defaultdict(set)
for u, mid in zip(test_pos["userId"].values, test_pos["movieId"].values):
    truth_by_user[u].add(mid)
pop_counts = train_pos.groupby("movieId")["userId"].count().sort_values(ascending=False)
pop_ranking = pop_counts.index.to_numpy()

user_items_csr = user_item
def recommend_items_for_uid(u_external: int, k: int = 100) -> List[int]:
    """
    Recommend top-k movieIds for a given external userId.
    - Uses ALS if user is known to the model.
    - Filters already-liked items from TRAIN to avoid leakage.
    - Falls back to global popularity if user not in training index.
    """
    if u_external not in uid2idx:
        seen = set(train_df.loc[train_df["userId"] == u_external, "movieId"].values)
        recs = [int(m) for m in pop_ranking if m not in seen]
        return recs[:k]

    u = uid2idx[u_external]

    try:
        single_user_row = user_items_csr[u]
        recs = als.recommend(
            u,
            single_user_row,
            N=k,
            filter_already_liked_items=True
        )
        rec_item_ids = [int(item_ids[iid]) for iid, _ in recs]
    except Exception as e:
        seen = set(train_df.loc[train_df["userId"] == u_external, "movieId"].values)
        rec_item_ids = [int(m) for m in pop_ranking if m not in seen][:k]

    return rec_item_ids

eval_users = [u for u in truth_by_user.keys() if (u in uid2idx) or (u in train_df["userId"].values)]
Ks = [5, 10, 20]

agg = {f"precision@{k}":[] for k in Ks}
agg.update({f"recall@{k}":[] for k in Ks})
agg.update({f"ndcg@{k}":[] for k in Ks})
agg.update({f"map@{k}":[] for k in Ks})

for u in eval_users:
    truth = truth_by_user[u]
    recs = recommend_items_for_uid(u, k=max(Ks))
    for k in Ks:
        agg[f"precision@{k}"].append(precision_at_k(recs, truth, k))
        agg[f"recall@{k}"].append(recall_at_k(recs, truth, k))
        agg[f"ndcg@{k}"].append(ndcg_at_k(recs, truth, k))
        agg[f"map@{k}"].append(apk(recs, truth, k))

summary = {m: float(np.mean(v)) if len(v)>0 else float('nan') for m,v in agg.items()}
print(tabulate(sorted(summary.items()), headers=["The metrics","The means"], floatfmt=".4f"))

#task 2
train_user_pos = train_pos.groupby("userId")["movieId"].nunique()
def bucket(n):
    if n < 10: return "cold(<10)"
    elif n < 50: return "warm(10-49)"
    else: return "hot(50+)"
buckets = train_user_pos.apply(bucket).to_dict()
rows = []
for u in eval_users:
    truth = truth_by_user[u]
    recs = recommend_items_for_uid(u, k=K)
    b = buckets.get(u, "unknown")
    rows.append({
        "userId": u,
        "bucket": b,
        "p@20": precision_at_k(recs, truth, 20),
        "r@20": recall_at_k(recs, truth, 20),
        "ndcg@20": ndcg_at_k(recs, truth, 20),
        "map@20": apk(recs, truth, 20),
        "train_positives": int(train_user_pos.get(u, 0))
    })

seg = pd.DataFrame(rows)
seg_summary = seg.groupby("bucket")[["p@20","r@20","ndcg@20","map@20"]].mean().reset_index()
display(seg_summary)
seg_summary.to_csv(os.path.join(OUT_DIR, "subpopulation_summary.csv"), index=False)

ax = seg_summary.plot(x="bucket", y="ndcg@20", kind="bar", legend=True, title="NDCG@20 by User Activity Bucket")
ax.set_xlabel("The Bucket"); ax.set_ylabel("The NDCG@20"); plt.tight_layout(); plt.show()

N_MINUTES = 60
TOPK_FOR_RESPONSE = 20
req_rows, resp_rows = [], []
base_ts = int(cutoff_ts)

for i, u in enumerate(eval_users):
    ts = base_ts + 60*(i+1)
    req_rows.append({"ts": ts, "user_id": int(u), "k": TOPK_FOR_RESPONSE})
    recs = recommend_items_for_uid(u, k=TOPK_FOR_RESPONSE)
    resp_rows.append({
        "ts": ts+1, "user_id": int(u), "status": 200, "latency_ms": int(np.random.randint(40,120)),
        "k": TOPK_FOR_RESPONSE, "movie_ids": json.dumps(recs), "model_version": "als_v1"
    })

reco_requests  = pd.DataFrame(req_rows)
reco_responses = pd.DataFrame(resp_rows)

wtest = test_df[(test_df["rating"] >= RATING_THRESHOLD) & (test_df["movieId"].isin(item_ids))].copy()
wtest["ts"] = (wtest["timestamp"] // 60) * 60
watch_events = wtest[["ts","userId","movieId"]].rename(columns={"userId":"user_id","movieId":"movie_id"})

reco_requests.to_csv(os.path.join(OUT_DIR, "reco_requests.csv"), index=False)
reco_responses.to_csv(os.path.join(OUT_DIR, "reco_responses.csv"), index=False)
watch_events.to_csv(os.path.join(OUT_DIR, "watch_events.csv"), index=False)

print("The Logs written:", OUT_DIR)
reco_responses.head()

resp = reco_responses.copy()
resp["movie_ids_list"] = resp["movie_ids"].apply(lambda s: list(map(int, json.loads(s))) if isinstance(s, str) else [])
resp["ts_end"] = resp["ts"] + N_MINUTES*60

from collections import defaultdict
watch_index = defaultdict(list)
for row in watch_events.itertuples(index=False):
    watch_index[row.user_id].append((int(row.ts), int(row.movie_id)))

def response_success(row) -> int:
    u = row.user_id
    t0, t1 = int(row.ts), int(row.ts_end)
    candidates = set(row.movie_ids_list)
    for ts, mid in watch_index.get(u, []):
        if t0 <= ts <= t1 and mid in candidates:
            return 1
    return 0

resp["success"] = resp.apply(response_success, axis=1)

overall_success = resp["success"].mean() if len(resp)>0 else float('nan')
latency_p50 = float(np.percentile(resp["latency_ms"], 50)) if len(resp)>0 else float('nan')
latency_p95 = float(np.percentile(resp["latency_ms"], 95)) if len(resp)>0 else float('nan')

print(f"Proxy success within {N_MINUTES} minutes: {overall_success:.4f}")
print(f"Latency p50: {latency_p50:.1f} ms | p95: {latency_p95:.1f} ms")

def success_at_k(k: int) -> float:
    tmp = resp.copy()
    tmp["movie_ids_list"] = tmp["movie_ids_list"].apply(lambda xs: xs[:k])
    tmp["success_k"] = tmp.apply(response_success, axis=1)
    return tmp["success_k"].mean()

for k in [5,10,20]:
    print(f"The Success@{k}: {success_at_k(k):.4f}")

print("Artifacts:")
print("The Model Registry:", REGISTRY)
print("The Eval Outputs:", OUT_DIR)

